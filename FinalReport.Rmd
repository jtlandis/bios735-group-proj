---
title: "Modeling Promotional Pasta Sales with Hierarchical Poisson Autoregression"
author: "Group 2"
output: 
  pdf_document: 
    toc: true
    number_sections: true
  html_document: 
    toc: true
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
library(tidyverse)
#library(pastasales) 
```

# Introduction
We analyze pasta sales data from an Italian grocery store, consisting of 116 items across four brands, observed daily from 2014 to 2018. Our goal is to model item-level sales with dependence on past observations, promotional status, and latent brand effects.

We compare likelihood-based models (EM, Newton-Raphson), Bayesian inference (MCMC), and machine learning benchmarks (Random Forests, Hidden Markov Models), using predictive accuracy as the main evaluation metric.

## Background

## Goals

# Data Description
```{r data-summary}
data("data_set_tidy")
glimpse(data_set_tidy)
```
Each row corresponds to daily sales for a specific item, along with brand and promotion indicators.

# Model Derivation

## Extended Poisson Autoregressive Model

Let the sales outcome for item $i$ at time $t$ be denoted by $y_{it}$, where:

$$
i \in \{1,\ldots,n\}, \quad t \in \{1,\ldots,T\}, \quad g_i \in \{1,\ldots,B\}
$$

Here, $g_i$ denotes the brand (group) to which item $i$ belongs.

We define the model as:

$$
y_{it} \mid m_{it} \sim \text{Poisson}(m_{it})
$$

$$
m_{it} = \sum_{l=1}^q \beta_{i,l} y_{i,t-l} + 
\left(1 - \sum_{l=1}^q \beta_{i,l} \right)
\exp\left( \mu_{g_i} + \eta_i + \mathbf{x}_{it}^\top \boldsymbol{\gamma}_i + f_t \right)
$$

**Model components:**

* $\beta_{i,l}$: autoregressive coefficients for item $i$
* $\mu_{g_i}$: brand-level intercept
* $\eta_i$: item-specific deviation
* $\boldsymbol{\gamma}_i$: covariate effects (e.g., promotion)
* $f_t$: global time-specific effect (e.g., seasonality)	

## Likelihood Function

Let $\boldsymbol{\theta}$ denote the full set of model parameters. The full likelihood across all items and times is:

$$
\mathcal{L}(\boldsymbol{\theta}) =
\prod_{i=1}^n \prod_{t=q+1}^T
\frac{m_{it}^{y_{it}} e^{-m_{it}}}{y_{it}!}
$$

The corresponding log-likelihood, used for optimization and posterior inference, is:

$$
\log \mathcal{L}(\boldsymbol{\theta}) =
\sum_{i=1}^n \sum_{t=q+1}^T \left[
y_{it} \log(m_{it}) - m_{it} - \log(y_{it}!)
\right]
$$

Let $\boldsymbol{\theta}$ denote the full set of model parameters. The complete-data likelihood is:

$$
\mathcal{L}(\boldsymbol{\theta}) =
\prod_{i=1}^n \prod_{t=q+1}^T
\frac{m_{it}^{y_{it}} e^{-m_{it}}}{y_{it}!}
$$

The log-likelihood is:

$$
\log \mathcal{L}(\boldsymbol{\theta}) =
\sum_{i=1}^n \sum_{t=q+1}^T \left[
y_{it} \log(m_{it}) - m_{it} - \log(y_{it}!)
\right]
$$

## Gradients

Gradient w.r.t. $\beta_l$:

$$
\frac{\partial m_t}{\partial \beta_l} = y_{t-l} - \exp(\mu + \mathbf{x}_t^\top \boldsymbol{\gamma})
$$

Gradient w.r.t. $\gamma_j$:

$$
\frac{\partial m_t}{\partial \gamma_j} = (1 - \sum \beta_l) \cdot x_{t,j} \cdot \exp(\mu + \mathbf{x}_t^\top \boldsymbol{\gamma})
$$

Now plug into:

$$
\frac{\partial \log \mathcal{L}}{\partial \theta} = \sum_{t=q+1}^T \left( \frac{y_t}{m_t} - 1 \right) \cdot \frac{\partial m_t}{\partial \theta}
$$


# Estimation Methods

## EM Algorithm
* E-step: Compute expected values of latent variables (e.g., item intercepts $\eta_i$)
* M-step: Maximize the expected log-likelihood with respect to model parameters

## Newton-Raphson
* Use gradients and Hessians of the log-likelihood to iteratively update parameters

## Bayesian Inference (MCMC)
* Fit the full hierarchical model in Stan
* Use priors on all parameters, including hierarchical priors:

$$
\boldsymbol{\gamma}i \sim \mathcal{N}(\boldsymbol{\mu}{g_i}, \rho_{g_i} \mathbf{I}), \quad
\mu_{g_i} \sim \mathcal{N}(0, \sigma_\mu^2), \quad
\eta_i \sim \mathcal{N}(0, \sigma_\eta^2)
$$

# Model Evaluation and Predictive Performance

We evaluate models using holdout-based prediction: the final 10–20% of each item’s time series is reserved for testing.

```{r eval-metrics}
# Example: results <- evaluate_holdout(...)
# knitr::kable(results)
```

** Evaluation Metrics **
 - Root Mean Squared Error (RMSE)
 - Mean Absolute Error (MAE)
 - Log Predictive Density (for Bayesian models)
 - Posterior predictive interval coverage (for MCMC)

# Machine Learning Benchmarks

## Random Forest
```{r rf-model}
# rf_fit <- fit_random_forest(...)
```

## Hidden Markov Model
```{r hmm-model}
# hmm_fit <- fit_hmm(...)
```

# Summary and Discussion
 - Which methods give the most accurate predictions for sparse, autocorrelated data?
 - What is the trade-off between hierarchical shrinkage and item-specific flexibility?
 - Are machine learning models (e.g., random forests) better suited for forecasting than structured generative models?
 