---
title: "Modeling Promotional Pasta Sales with Hierarchical Poisson Autoregression"
author: "Group 2"
output: 
  html_document: 
    toc: true
    number_sections: true
  pdf_document: 
    toc: true
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
library(tidyverse)
#library(pastasales) 
```

# Introduction

We have identified a dataset of pasta sales from an Italian grocery store with observations made between January 2014 and December 2018. This dataset contains the quantity of each item sold for 1,798 time points. Along with this quantity of sales, we are provided with a brand identifier and a binary label of whether the item was promoted on that day as well. This time series data is thus hierarchical, with four brands and up to 45 unique items in each brand.

The goal of this report is to investigate the following:

-   How do temporal trends and promotion-sales relationships vary across the four pasta brands?
-   Can we improve forecasting of sales by accounting for the hierarchical structure in the data compared to simply modeling each brand (and item) independently?



We compare likelihood-based models (EM, Newton-Raphson), Bayesian inference (MCMC), and machine learning benchmarks (Random Forests, Hidden Markov Models), using predictive accuracy as the main evaluation metric.

## Background

## Goals

# Data Description

```{r data-summary}
data("data_set_tidy")
glimpse(data_set_tidy)
```

Each row corresponds to daily sales for a specific item, along with brand and promotion indicators.

# Model Derivation

## Extended Poisson Autoregressive Model

Let the sales outcome for item $i$ at time $t$ be denoted by $y_{it}$, where:

$$
i \in \{1,\ldots,n\}, \quad t \in \{1,\ldots,T\}, \quad g_i \in \{1,\ldots,B\}
$$

Here, $g_i$ denotes the brand (group) to which item $i$ belongs.

We define the model as:

$$
y_{t} \sim \text{Poisson}(m_{t})
$$

$$
m_t = \sum_{l=1}^{q} \beta_{l} y_{t-l} +
\left(1 - \sum_{l=1}^{q} \beta_{l} \right) \cdot \exp(\mathbf{x}_t^\top \boldsymbol{\gamma})
$$

Where:

-   $\beta_{l}$: AR coefficients (constrained so sum $\leq$ 1)
-   $\gamma$: covariate effect vector

## Likelihood Function

Let $\boldsymbol{\theta}$ denote the full set of model parameters. The full likelihood across all items and times is:

$$
\mathcal{L}(\boldsymbol{\theta}) =
\prod_{t=q+1}^T
\frac{m_{t}^{y_{t}} e^{-m_{t}}}{y_{t}!}
$$

The corresponding log-likelihood, used for optimization and posterior inference, is:

$$
\log \mathcal{L}(\boldsymbol{\theta}) =
\sum_{t=q+1}^T \left[
y_{t} \log(m_{t}) - m_{t} - \log(y_{t}!)
\right]
$$ Where,

$$
m_t = \underbrace{\sum_{l=1}^{q} \beta_l y_{t-l}}_{\text{AR part}} +
\underbrace{\left(1 - \sum{l=1}^{q} \beta_l \right)}_{\text{mixing weight}} \cdot
\underbrace{\exp(\mathbf{x}t^\top \boldsymbol{\gamma})}_{\text{covariate part}}
$$

## Gradient (Score Function)

Define the following:

-   $a_t = \sum_{l=q}^q \beta_ty_{t-l}$
-   $c_t = \exp(x_t^T\gamma)$
-   $w = 1 - \sum\beta_t$
-   $m_t = a_t + w \cdot c_t$

Then, the derivative of the log-likelihood with respect to $\gamma_j$ is: $$
\frac{\partial{\log\mathcal{L}}}{\partial\gamma_j} = \sum_{t = q+ l}^T \left[ \frac{y_t}{m_t} - 1\right] \cdot w \cdot c_t \cdot x_{tj}
$$

And the derivative of the log-likelihood with respect to $\beta_k$ is: $$
\frac{\partial{\log\mathcal{L}}}{\partial\beta_k} = \sum_{t = q+ l}^T \left[ \frac{y_t}{m_t} - 1\right] \left[ y_{t-k} - c_t\right]
$$

# Estimation Methods

## EM Algorithm

-   E-step: Applicable if we add new random intercepts (either item or brand level)
-   M-step: Maximize the expected log-likelihood with respect to model parameters $\gamma$, $\beta$, and $\tau$


## Bayesian Inference (MCMC)

-   Fit the independent Poisson Autoregression (PAR) model for each item with MCMC (currently implemented in STAN)

-   Place priors directly on model parameters

-   $\tau$: mixing weight for AR component, $\beta = \tau \tilde{\boldsymbol{\beta}}$

Priors:

-   $\boldsymbol{\gamma} \sim \mathcal{N}(\boldsymbol{\mu}, \Sigma_\gamma)$
-   $\Sigma_\gamma \sim \text{Inv-Wishart}(\nu, \Psi)$ — (approximated as fixed in Stan)
-   $\tilde{\boldsymbol{\beta}} \sim \text{Dirichlet}(\alpha)$
-   $\tau \sim \text{Beta}(a, b)$

# Model Evaluation and Predictive Performance

We evaluate models using holdout-based prediction: the final 10–20% of each item’s time series is reserved for testing.

```{r eval-metrics}
# Example: results <- evaluate_holdout(...)
# knitr::kable(results)
```

\*\* Evaluation Metrics \*\* - Root Mean Squared Error (RMSE) - Mean Absolute Error (MAE) - Log Predictive Density (for Bayesian models) - Posterior predictive interval coverage (for MCMC)

# Machine Learning Benchmarks

## Random Forest

```{r rf-model}
# rf_fit <- fit_random_forest(...)
```

# Summary and Discussion

-   Which methods give the most accurate predictions for sparse, autocorrelated data?
-   What is the trade-off between hierarchical shrinkage and item-specific flexibility?
-   Are machine learning models (e.g., random forests) better suited for forecasting than structured generative models?
