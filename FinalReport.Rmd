---
title: "Modeling Promotional Pasta Sales with Hierarchical Poisson Autoregression"
author: "Group 2"
output: 
  pdf_document: 
    toc: true
    number_sections: true
  html_document: 
    toc: true
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
library(tidyverse)
#library(pastasales) 
```

# Introduction
We analyze pasta sales data from an Italian grocery store, consisting of 116 items across four brands, observed daily from 2014 to 2018. Our goal is to model item-level sales with dependence on past observations, promotional status, and latent brand effects.

We compare likelihood-based models (EM, Newton-Raphson), Bayesian inference (MCMC), and machine learning benchmarks (Random Forests, Hidden Markov Models), using predictive accuracy as the main evaluation metric.

## Background

## Goals

# Data Description
```{r data-summary}
data("data_set_tidy")
glimpse(data_set_tidy)
```
Each row corresponds to daily sales for a specific item, along with brand and promotion indicators.

# Model Derivation

## Extended Poisson Autoregressive Model

Let the sales outcome for item $i$ at time $t$ be denoted by $y_{it}$, where:

$$
i \in \{1,\ldots,n\}, \quad t \in \{1,\ldots,T\}, \quad g_i \in \{1,\ldots,B\}
$$

Here, $g_i$ denotes the brand (group) to which item $i$ belongs.

We define the model as:

$$
y_{t} \sim \text{Poisson}(m_{t})
$$

$$
m_t = \sum_{l=1}^{q} \beta_{l} y_{t-l} +
\left(1 - \sum_{l=1}^{q} \beta_{l} \right) \cdot \exp(\mathbf{x}_t^\top \boldsymbol{\gamma})
$$

Where:

* $\beta_{l}$: AR coefficients (constrained so sum $\leq$ 1)
* $\gamma$: covariate effect vector

## Likelihood Function

Let $\boldsymbol{\theta}$ denote the full set of model parameters. The full likelihood across all items and times is:

$$
\mathcal{L}(\boldsymbol{\theta}) =
\prod_{t=q+1}^T
\frac{m_{t}^{y_{t}} e^{-m_{t}}}{y_{t}!}
$$

The corresponding log-likelihood, used for optimization and posterior inference, is:

$$
\log \mathcal{L}(\boldsymbol{\theta}) =
\sum_{t=q+1}^T \left[
y_{t} \log(m_{t}) - m_{t} - \log(y_{t}!)
\right]
$$
Where, 

$$

m_t = \underbrace{\sum_{l=1}^{q} \beta_l y_{t-l}}_{\text{AR part}} +
\underbrace{\left(1 - \sum{l=1}^{q} \beta_l \right)}_{\text{mixing weight}} \cdot
\underbrace{\exp(\mathbf{x}t^\top \boldsymbol{\gamma})}_{\text{covariate part}}

$$


## Gradient (Score Function)

Define the following: 

* $a_t = \sum_{l=q}^q \beta_ty_{t-l}$
* $c_t = \exp(x_t^T\gamma)$
* $w = 1 - \sum\beta_t$
* $m_t = a_t + w \cdot c_t$

Then, the derivative of the log-likelihood with respect to $\gamma_j$ is:
$$
\frac{\partial{\log\mathcal{L}}}{\partial\gamma_j} = \sum_{t = q+ l}^T \left[ \frac{y_t}{m_t} - 1\right] \cdot w \cdot c_t \cdot x_{tj}
$$

And the derivative of the log-likelihood with respect to $\beta_k$ is: 
$$
\frac{\partial{\log\mathcal{L}}}{\partial\beta_k} = \sum_{t = q+ l}^T \left[ \frac{y_t}{m_t} - 1\right] \left[ y_{t-k} - c_t\right]
$$

# Estimation Methods

## EM Algorithm
* E-step: Applicable if we add new random intercepts (either item or brand level)
* M-step: Maximize the expected log-likelihood with respect to model parameters $\gamma$, $\beta$, and $\tau$

## Newton-Raphson
* Use gradients and Hessians of the log-likelihood to iteratively update parameters

## Bayesian Inference (MCMC)
* Fit the independent Poisson Autoregression (PAR) model for each item with MCMC (currently implemented in STAN)
* Place priors directly on model parameters

* $\tau$: mixing weight for AR component, $\beta = \tau \tilde{\boldsymbol{\beta}}$

Priors:

* $\boldsymbol{\gamma} \sim \mathcal{N}(\boldsymbol{\mu}, \Sigma_\gamma)$
* $\Sigma_\gamma \sim \text{Inv-Wishart}(\nu, \Psi)$ — (approximated as fixed in Stan)
* $\tilde{\boldsymbol{\beta}} \sim \text{Dirichlet}(\alpha)$
* $\tau \sim \text{Beta}(a, b)$

# Model Evaluation and Predictive Performance

We evaluate models using holdout-based prediction: the final 10–20% of each item’s time series is reserved for testing.

```{r eval-metrics}
# Example: results <- evaluate_holdout(...)
# knitr::kable(results)
```

** Evaluation Metrics **
 - Root Mean Squared Error (RMSE)
 - Mean Absolute Error (MAE)
 - Log Predictive Density (for Bayesian models)
 - Posterior predictive interval coverage (for MCMC)

# Machine Learning Benchmarks

## Random Forest
```{r rf-model}
# rf_fit <- fit_random_forest(...)
```

## Hidden Markov Model
```{r hmm-model}
# hmm_fit <- fit_hmm(...)
```

# Summary and Discussion
 - Which methods give the most accurate predictions for sparse, autocorrelated data?
 - What is the trade-off between hierarchical shrinkage and item-specific flexibility?
 - Are machine learning models (e.g., random forests) better suited for forecasting than structured generative models?
 