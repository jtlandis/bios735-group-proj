---
title: "Modeling Promotional Pasta Sales with Hierarchical Poisson Autoregression"
author: "Group 2"
output: 
  pdf_document: 
    toc: true
    number_sections: true
  html_document: 
    toc: true
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
library(tidyverse)
#library(pastasales) 
```

# Introduction
We analyze pasta sales data from an Italian grocery store, consisting of 116 items across four brands, observed daily from 2014 to 2018. Our goal is to model item-level sales with dependence on past observations, promotional status, and latent brand effects.

We compare likelihood-based models (EM, Newton-Raphson), Bayesian inference (MCMC), and machine learning benchmarks (Random Forests, Hidden Markov Models), using predictive accuracy as the main evaluation metric.

## Background

## Goals

# Data Description
```{r data-summary}
data("data_set_tidy")
glimpse(data_set_tidy)
```
Each row corresponds to daily sales for a specific item, along with brand and promotion indicators.

# Model Derivation

## Extended Poisson Autoregressive Model

Let the sales outcome for item $i$ at time $t$ be denoted by $y_{it}$, where:

$$
i \in \{1,\ldots,n\}, \quad t \in \{1,\ldots,T\}, \quad g_i \in \{1,\ldots,B\}
$$

Here, $g_i$ denotes the brand (group) to which item $i$ belongs.

We define the model as:

$$
y_{t} \sim \text{Poisson}(m_{t})
$$

$$
m_t = \sum_{l=1}^{q} \beta_{l} y_{t-l} +
\left(1 - \sum_{l=1}^{q} \beta_{l} \right) \cdot \exp(\mathbf{x}_t^\top \boldsymbol{\gamma})
$$

Where:

* $\beta_{l}$: AR coefficients (constrained so sum $\leq$ 1)
* $\gamma$: covariate effect vector

## Likelihood Function

Let $\boldsymbol{\theta}$ denote the full set of model parameters. The full likelihood across all items and times is:

$$
\mathcal{L}(\boldsymbol{\theta}) =
\prod_{t=q+1}^T
\frac{m_{t}^{y_{t}} e^{-m_{t}}}{y_{t}!}
$$

The corresponding log-likelihood, used for optimization and posterior inference, is:

$$
\log \mathcal{L}(\boldsymbol{\theta}) =
\sum_{t=q+1}^T \left[
y_{t} \log(m_{t}) - m_{t} - \log(y_{t}!)
\right]
$$
Where, 

$$

m_t = \underbrace{\sum_{l=1}^{q} \beta_l y_{t-l}}_{\text{AR part}} +
\underbrace{\left(1 - \sum{l=1}^{q} \beta_l \right)}_{\text{mixing weight}} \cdot
\underbrace{\exp(\mathbf{x}t^\top \boldsymbol{\gamma})}_{\text{covariate part}}

$$


## Gradients

Gradient w.r.t. $\beta_l$:

$$
\frac{\partial m_t}{\partial \beta_l} = y_{t-l} - \exp(\mu + \mathbf{x}_t^\top \boldsymbol{\gamma})
$$

Gradient w.r.t. $\gamma_j$:

$$
\frac{\partial m_t}{\partial \gamma_j} = (1 - \sum \beta_l) \cdot x_{t,j} \cdot \exp(\mu + \mathbf{x}_t^\top \boldsymbol{\gamma})
$$

Now plug into:

$$
\frac{\partial \log \mathcal{L}}{\partial \theta} = \sum_{t=q+1}^T \left( \frac{y_t}{m_t} - 1 \right) \cdot \frac{\partial m_t}{\partial \theta}
$$


# Estimation Methods

## EM Algorithm
* E-step: Compute expected values of latent variables (e.g., item intercepts $\eta_i$)
* M-step: Maximize the expected log-likelihood with respect to model parameters

## Newton-Raphson
* Use gradients and Hessians of the log-likelihood to iteratively update parameters

## Bayesian Inference (MCMC)
* Fit the full hierarchical model in Stan
* Use priors on all parameters, including hierarchical priors:

* $\tau$: mixing weight for AR component

Priors:

* $\boldsymbol{\gamma} \sim \mathcal{N}(\boldsymbol{\mu}, \Sigma_\gamma)$
* $\Sigma_\gamma \sim \text{Inv-Wishart}(\nu, \Psi)$ — (approximated as fixed in Stan)
* $\tilde{\boldsymbol{\beta}} \sim \text{Dirichlet}(\alpha)$, $\beta = \tau \tilde{\boldsymbol{\beta}}$
* $\tau \sim \text{Beta}(a, b)$

# Model Evaluation and Predictive Performance

We evaluate models using holdout-based prediction: the final 10–20% of each item’s time series is reserved for testing.

```{r eval-metrics}
# Example: results <- evaluate_holdout(...)
# knitr::kable(results)
```

** Evaluation Metrics **
 - Root Mean Squared Error (RMSE)
 - Mean Absolute Error (MAE)
 - Log Predictive Density (for Bayesian models)
 - Posterior predictive interval coverage (for MCMC)

# Machine Learning Benchmarks

## Random Forest
```{r rf-model}
# rf_fit <- fit_random_forest(...)
```

## Hidden Markov Model
```{r hmm-model}
# hmm_fit <- fit_hmm(...)
```

# Summary and Discussion
 - Which methods give the most accurate predictions for sparse, autocorrelated data?
 - What is the trade-off between hierarchical shrinkage and item-specific flexibility?
 - Are machine learning models (e.g., random forests) better suited for forecasting than structured generative models?
 